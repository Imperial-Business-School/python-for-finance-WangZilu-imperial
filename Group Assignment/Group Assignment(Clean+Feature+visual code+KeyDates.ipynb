{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DoorDash Stock Analysis (Cleaning Version)\n",
    "\n",
    "Import the basic function package `pandas`, `numpy`, `matplotlib.pyplot` and load the data file `DASH_A1.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DASH_A1.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Viewing and Preprocessing\n",
    "For easy reading, change the format of the `Date` and thus can be quickly indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Date = pd.to_datetime(df.Date, format=\"%d-%m-%Y\")\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the table by date in ascending order\n",
    "df.index.is_monotonic_increasing # check whether the current data is in the correct order\n",
    "df.sort_index(inplace=True)  \n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether there is duplicated data, if so, drop them\n",
    "df.duplicated().sum() # there are 50 duplicated datas\n",
    "\n",
    "df.drop_duplicates(inplace=True) \n",
    "df\n",
    "\n",
    "df.duplicated().sum() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the total number of missing values in the dataframe for each column \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **23** missing value \"Close\"; **24** missing value \"High\", **15** missing value \"Low\", **15** missing value \"Open\", **26** missing value \"Volume\".\n",
    "\n",
    "Then can find out which rows have missing data by using `isnull()`, `any()` along rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = df.isnull().any(axis=1)\n",
    "df[condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Since all 5 columns have some missing values, we then use different approaches to fill the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Price\n",
    "Fill missing values in the Open column **with the Close of the day before** as an approximation, ignoring overnight trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Open'] = df['Open'].fillna(df['Close'].shift(1))\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Price\n",
    "Fill missing values in the Close column with the methods **forward-filled** to avoid look-ahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close'] = df['Close'].ffill()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High and Low Price\n",
    "Fill missing values in the High and Low columns **with the mean of the respective High or Low within that month**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Month\"] = df.index.to_period(\"M\")\n",
    "df\n",
    "\n",
    "df[\"High\"] = df[\"High\"].fillna(df.groupby(\"Month\")[\"High\"].transform(\"mean\"))\n",
    "df[\"Low\"] = df[\"Low\"].fillna(df.groupby(\"Month\")[\"Low\"].transform(\"mean\"))\n",
    "df.isnull().sum()\n",
    "\n",
    "#for col in [\"High\", \"Low\"]:\n",
    "    #for period in df[\"Month\"].unique():\n",
    "        #mask = df[\"Month\"] == period\n",
    "        #monthly_mean = df.loc[mask, col].mean()\n",
    "        #df.loc[mask, col] = df.loc[mask, col].fillna(monthly_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume (Zero Volume Condition)\n",
    "**If the Close price is equal to the Open price, fill the missing values in the Volume column with zero**, indicating\n",
    "no change in trading activity for the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_equal = (df.Volume.isnull()) & (df.Close == df.Open)\n",
    "df.loc[condition_equal, \"Volume\"] = 0\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume (Non-zero Volume Condition)\n",
    "**If the Close price is not equal to the Open price, fill the missing values in the Volume column with the median of the existing Volume values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_notequal = (df['Volume'].isnull()) & (df['Close'] != df['Open']) \n",
    "df.loc[condition_notequal, 'Volume'] = df['Volume'].median()\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum() # double check there is no missing value\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Daily Returns\n",
    "Calculate the simple daily returns to measure the day-to-day percentage change in the Close prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SimpleDailyReturns\"] = df.Close.pct_change()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic Returns\n",
    "Calculate the logarithmic returns using Close prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20-Day Momentum\n",
    "Calculate the 20-day momentum by subtracting the Close price 20 days prior from the current Close price, providing insights into the stock's short-term trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['20Day_Momentum'] = df['Close'] - df['Close'].shift(20)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20-Day Simple Moving Average\n",
    "Calculate the 20-day simple moving average to smooth out short-term fluctuations and highlight longer-term trends in the Close prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['20Day_SMA'] = df['Close'].rolling(window=20).mean()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20-Day Rolling Volatility\n",
    "Calculate the 20-day rolling volatility based on the standard deviation of simple daily returns to indicate the stock's risk level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['20Day_Volatility'] = df['SimpleDailyReturns'].rolling(window=20).std()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day of the Week\n",
    "Identify the day of the week for each trading day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Day_of_Week'] = df.index.day_name()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Surge Identification: \n",
    "Identify days where the price surged, defined as when the daily return is more than 4 standard deviations above the mean daily return for the period, indicating significant price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criteria: Simple Return > Mean + 4 * Std Dev\n",
    "mean_return = df['SimpleDailyReturns'].mean()\n",
    "std_return = df['SimpleDailyReturns'].std()\n",
    "df['Price_Surge'] = df['SimpleDailyReturns'] > (mean_return + 4 * std_return)\n",
    "df\n",
    "\n",
    "surge_days = df[df['Price_Surge'] == True]\n",
    "surge_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume Spike Identification\n",
    "Identify days where the volume spiked, defined as when the trading volume is more than 6 standard deviations above the mean volume for the period, highlighting unusual trading activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criteria: Volume > Mean + 6 * Std Dev\n",
    "mean_volume = df['Volume'].mean()\n",
    "std_volume = df['Volume'].std()\n",
    "df['Volume_Spike'] = df['Volume'] > (mean_volume + 6 * std_volume)\n",
    "\n",
    "unusualtrading_days = df[df['Volume_Spike'] == True]\n",
    "unusualtrading_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bollinger Bands Calculation\n",
    "Calculate the upper and lower Bollinger Bands for the stock, which are set at 2 standard deviations above and below the 20- day simple moving average, to identify overbought and oversold conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Dev'] = df['Close'].rolling(window=20).std()\n",
    "\n",
    "df['Upper_BB'] = df['20Day_SMA'] + 2 * df['Dev']\n",
    "df['Lower_BB'] = df['20Day_SMA'] - 2 * df['Dev']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.title(\"Bollinger on DoorDash\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price(USD)\")\n",
    "\n",
    "plt.plot(df.Upper_BB, label=\"Upper Bollinger Band\", color=\"grey\", linestyle=\"--\")\n",
    "plt.plot(df.Lower_BB, label=\"Lower Bollinger Band\", color=\"grey\",linestyle=\"--\")\n",
    "plt.fill_between(df.index, df.Upper_BB, df.Lower_BB, color=\"grey\", alpha=0.5) \n",
    "plt.plot(df.Close, label=\"Closing Price\")\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Dates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Volatility Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the Year column exists\n",
    "df['Year'] = df.index.year\n",
    "\n",
    "# Remove missing values in 20-day volatility\n",
    "df_vol = df.dropna(subset=['20Day_Volatility'])\n",
    "\n",
    "# Find the index of the max volatility in each year\n",
    "date_vol_max = df_vol.groupby('Year')['20Day_Volatility'].idxmax()\n",
    "\n",
    "\n",
    "# Select those rows from the original dataframe\n",
    "highest_vol_days = df.loc[date_vol_max, ['Year', '20Day_Volatility']]\n",
    "# Display the results\n",
    "print(\"Highest Volatility Day per Year:\")\n",
    "print(highest_vol_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Largest Price Surge Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the Year column exists\n",
    "df['Year'] = df.index.year\n",
    "\n",
    "# Remove missing values in simple return\n",
    "df_ret = df.dropna(subset=['SimpleDailyReturns'])\n",
    "\n",
    "# Get the index of the highest daily return in each year\n",
    "date_surge_max = df_ret.groupby('Year')['SimpleDailyReturns'].idxmax()\n",
    "\n",
    "# Retrieve those rows\n",
    "highest_surge_days = df.loc[date_surge_max, ['Year', 'SimpleDailyReturns']]\n",
    "\n",
    "# Display the results\n",
    "print(\"Largest Price Surge Day per Year:\")\n",
    "print(highest_surge_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Largest Price Drop Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the Year column exists\n",
    "df['Year'] = df.index.year\n",
    "\n",
    "# Get the index of the lowest daily return in each year\n",
    "date_drop_min = df_ret.groupby('Year')['SimpleDailyReturns'].idxmin()\n",
    "\n",
    "# Retrieve those rows\n",
    "largest_drop_days = df.loc[date_drop_min, ['Year', 'SimpleDailyReturns']]\n",
    "\n",
    "# Display the results\n",
    "print(\"Largest Price Drop Day per Year:\")\n",
    "print(largest_drop_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Volume Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = df.index.year\n",
    "\n",
    "unique_years = sorted(df['Year'].unique())\n",
    "print(f\"Unique years in dataset: {unique_years}\")\n",
    "# Years range from 2020 to 2025; we should therefore obtain 6 dates with the date and max volume within each year\n",
    "\n",
    "# To find the date where the Volume is maximised (for each year)\n",
    "date_Vol_max = df.groupby('Year')['Volume'].idxmax()\n",
    "\n",
    "highest_Vol_year = df.loc[date_Vol_max]\n",
    "\n",
    "print(highest_Vol_year[['Year', 'Volume']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest High and Lowest Low\n",
    "\n",
    "The results below illustrate the dates and values of DoorDash's highest and lowest stock prices per year since IPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest High (per year)\n",
    "date_High_max = df.groupby('Year')['High'].idxmax()\n",
    "\n",
    "# Lowest Low (per year)\n",
    "date_Low_min = df.groupby('Year')['Low'].idxmin()\n",
    "\n",
    "highest_high_year = df.loc[date_High_max]\n",
    "lowest_low_year = df.loc[date_Low_min]\n",
    "\n",
    "print(\"Highest High Each Year:\")\n",
    "print(highest_high_year[['Year', 'High']])\n",
    "\n",
    "print(\"\\nLowest Low Each Year:\")\n",
    "print(lowest_low_year[['Year', 'Low']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual Open and Close\n",
    "\n",
    "The results below show DoorDash's opening (at the start of each year) and close prices (at the end of each year).\n",
    "\n",
    "\n",
    "Including the exact date is optional as the *Open* must be the first trading day of the year (e.g., 2nd Jan), and the *Close* must be the last trading day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_Open_first = df.groupby('Year').head(1).index\n",
    "date_Close_last = df.groupby('Year').tail(1).index\n",
    "\n",
    "annual_open = df.loc[date_Open_first, ['Year', 'Open']]\n",
    "annual_close = df.loc[date_Close_last, ['Year', 'Close']]\n",
    "\n",
    "print(\"Annual Open Prices:\")\n",
    "print(annual_open.to_string(index = False))\n",
    "\n",
    "print(\"\\nAnnual Close Prices:\")\n",
    "print(annual_close.to_string(index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Trend Chart\n",
    "**Close Price with SMA and Bollinger Bands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df.index, df[\"Close\"], label=\"Close\", color=\"blue\", linewidth=1)\n",
    "plt.plot(df.index, df[\"20Day_SMA\"], label=\"20-Day SMA\", color=\"orange\", linewidth=1)\n",
    "plt.fill_between(df.index, df[\"Upper_BB\"], df[\"Lower_BB\"], color=\"gray\", alpha=0.5, label=\"Bollinger Bands\")\n",
    "plt.title(\"Close Price with SMA and Bollinger Bands\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price(USD)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Volume and Volatility Subplots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "ax1.plot(df.index, df[\"Volume\"], color=\"purple\")\n",
    "ax1.set_title(\"Daily Trading Volume\")\n",
    "ax1.set_ylabel(\"Volume\")\n",
    "ax2.plot(df.index, df[\"20Day_Volatility\"], color=\"green\")\n",
    "ax2.set_title(\"20-Day Rolling Volatility\")\n",
    "ax2.set_ylabel(\"Volatility\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Histogram of Log Returns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_bins = int(np.sqrt(len(df.Log_Returns)))\n",
    "recommended_bins # is 33\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.hist(df.Log_Returns, bins=33)\n",
    "plt.title(\"Histogram of Log Returns\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Log Returns\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df[\"Log_Returns\"].dropna(), bins=50, kde=True, color='teal')\n",
    "plt.title(\"Histogram of Log Returns\")\n",
    "plt.xlabel(\"Log Return\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plot Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scatter Plot: Volume vs Daily Return**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(df.SimpleDailyReturns, df.Volume)\n",
    "plt.title(\"Volume vs Daily Returns\")\n",
    "plt.ylabel(\"Volume\")\n",
    "plt.xlabel(\"Simple Daily Return\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
